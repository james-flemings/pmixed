{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import math\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([116635588])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "train_file = os.path.join(data_dir, \"wikitext-103-train-corpus.pt\")\n",
    "train = torch.load(train_file)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"GPT2\")\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained('GPT2', pad_token_id=tokenizer.eos_token_id).to(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/james/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 102.34it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset('wikitext', 'wikitext-103-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1801350\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/james/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-0373ac50873b497f_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/james/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-96928dfcd8be926a_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/james/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-bf8f68fe7370cf91_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1801350\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 526\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/james/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-09d3da81c9b157a4_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/james/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-bb5c35209c9889fd_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/james/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ae030aaeea65564e_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts, \n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 27912\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets[\"train\"].shard(num_shards=8, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/peft/peft_model.py\", line 678, in forward\n    return self.base_model(\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1080, in forward\n    transformer_outputs = self.transformer(\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 846, in forward\n    inputs_embeds = self.wte(input_ids)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n    return F.embedding(\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/functional.py\", line 2210, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 31\u001b[0m\n\u001b[1;32m     17\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     18\u001b[0m     model_name,\n\u001b[1;32m     19\u001b[0m     evaluation_strategy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     26\u001b[0m     model\u001b[39m=\u001b[39mlora_model,\n\u001b[1;32m     27\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     28\u001b[0m     train_dataset\u001b[39m=\u001b[39mlm_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\u001b[39m#[i*train_share: (i+1)*train_share],\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     eval_dataset\u001b[39m=\u001b[39mlm_datasets[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m#[i*val_share: (i+1)*val_share]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     32\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerplexity: \u001b[39m\u001b[39m{\u001b[39;00mmath\u001b[39m.\u001b[39mexp(eval_results[\u001b[39m'\u001b[39m\u001b[39meval_loss\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1937\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1938\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1940\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1942\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1943\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1944\u001b[0m ):\n\u001b[1;32m   1945\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/transformers/trainer.py:2759\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2758\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2759\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2761\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2762\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/transformers/trainer.py:2784\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2783\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2784\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2785\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     90\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/peft/peft_model.py\", line 678, in forward\n    return self.base_model(\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1080, in forward\n    transformer_outputs = self.transformer(\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 846, in forward\n    inputs_embeds = self.wte(input_ids)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n    return F.embedding(\n  File \"/home/james/priv_pred_ensemble/venv/lib/python3.10/site-packages/torch/nn/functional.py\", line 2210, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"
     ]
    }
   ],
   "source": [
    "num_ensemble = 8\n",
    "epochs = 10\n",
    "device = \"cuda:7\"\n",
    "train_share = lm_datasets['train'].num_rows // num_ensemble\n",
    "val_share = lm_datasets['validation'].num_rows // num_ensemble\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "for i in range(8):\n",
    "    lm_shards = {}\n",
    "    lm_shards['train'] = lm_datasets['train'].shard(num_shards=num_ensemble, index=i)\n",
    "    lm_shards['validation'] = lm_datasets['train'].shard(num_shards=num_ensemble, index=i)\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "    lm_datasets['train'],\n",
    "    batch_size=4\n",
    "    )\n",
    "\n",
    "    validation_loader = DataLoader(\n",
    "    lm_datasets['train'],\n",
    "    batch_size=4\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    lora_model = get_peft_model(pretrained_model, lora_config).to(device)\n",
    "    print(lora_model.device)\n",
    "\n",
    "    model_name = f\"lora-gpt2-{i}-finetuned-wikitext2\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = lora_model(batch)\n",
    "            loss, logits = outputs[:2]\n",
    "            preds = np.argmax(logits.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a085e5ca12e18b44f217294c7fed45579a01552cf87fd56b67c43c641f99aba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
